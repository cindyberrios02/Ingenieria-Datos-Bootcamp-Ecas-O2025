# AnÃ¡lisis de Caso  
## Procesamiento distribuido de datos con Apache Spark  


### ğŸ“ SituaciÃ³n inicial  
La empresa **DataStream Analytics S.A.** ofrece servicios de anÃ¡lisis de datos para el sector financiero.  
Actualmente, necesitan **generar reportes diarios** sobre los movimientos financieros de millones de registros provenientes de sucursales distribuidas en todo el paÃ­s.  

El sistema local utilizado hasta ahora no escala ni responde a la velocidad requerida. Por eso, el equipo de tecnologÃ­a decidiÃ³ migrar a una soluciÃ³n de **procesamiento distribuido con Apache Spark**.  


### ğŸ” DescripciÃ³n del caso  
Como Ingeniero/a de Datos, el desafÃ­o consiste en diseÃ±ar una soluciÃ³n con Spark que permita:  

- Procesar archivos de gran volumen.  
- Detectar patrones sospechosos para prevenir fraudes.  
- Asegurar **escalabilidad** y **tolerancia a fallos**.  


### 1. CreaciÃ³n del RDD  

```python
from pyspark import SparkContext

sc = SparkContext("local[*]", "AnalisisFinanciero")
rdd = sc.textFile("hdfs://ruta/datos_movimientos/*.csv")


textFile() carga datos de manera distribuida desde HDFS/S3.

Cada lÃ­nea se convierte en un registro dentro del RDD.

El trabajo se reparte automÃ¡ticamente entre los nodos del cluster.

2. Transformaciones propuestas

Filtrado de registros invÃ¡lidos o incompletos

rdd_filtrado = rdd.filter(lambda linea: len(linea.split(",")) == 6)


Mapeo y estructuraciÃ³n de datos

rdd_mapeado = rdd_filtrado.map(lambda linea: linea.split(","))


AgregaciÃ³n por usuario (ejemplo: sumar montos)

rdd_agrupado = rdd_mapeado.map(lambda x: (x[0], float(x[2]))).reduceByKey(lambda a, b: a + b)

3. Acciones propuestas

take() o collect() â†’ Obtener una muestra para validaciÃ³n.

muestra = rdd_agrupado.take(10)


saveAsTextFile() â†’ Exportar resultados procesados.

rdd_agrupado.saveAsTextFile("hdfs://ruta/resultados/reportes_diarios")

4. DefiniciÃ³n de un Job Spark

Un Job Spark es la unidad de ejecuciÃ³n que comienza cuando se llama a una acciÃ³n.
En este caso, un Job corresponde al flujo:

Leer archivos distribuidos.

Aplicar transformaciones (filtrado, mapeo, agregaciÃ³n).

Ejecutar la acciÃ³n final para guardar los reportes diarios.

5. ReflexiÃ³n

âœ… Importancia del procesamiento distribuido:

Procesar millones de registros rÃ¡pidamente.

Escalar horizontalmente agregando nodos.

Tolerancia a fallos con reintentos automÃ¡ticos.

Procesamiento en memoria â†’ mucho mÃ¡s rÃ¡pido que sistemas locales.

âš ï¸ DesafÃ­os tÃ©cnicos:

ConfiguraciÃ³n del cluster y recursos (memoria, cores, particiones).

Balanceo de carga entre nodos.

Limpieza y consistencia de los datos.

Costos de infraestructura si no se optimiza.

ğŸ“Š Esquema visual del flujo con Spark
   [Sucursales: Archivos CSV grandes]
                   â”‚
                   â–¼
          Ingesta en HDFS / S3
                   â”‚
                   â–¼
       SparkContext â†’ RDD inicial
                   â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                     â–¼
   [TransformaciÃ³n 1]   Filtrado de registros
   [TransformaciÃ³n 2]   Mapeo y estructuraciÃ³n
   [TransformaciÃ³n 3]   AgregaciÃ³n por usuario
                   â”‚
                   â–¼
             Acciones (Jobs)
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â–¼                     â–¼
   `collect()` (validaciÃ³n)   `saveAsTextFile()` (reportes diarios)



