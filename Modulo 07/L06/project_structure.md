# Estructura Completa del Proyecto MLlib E-commerce

## ğŸ“ Estructura de Directorios

```
mllib-ecommerce-analysis/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                              # DocumentaciÃ³n principal
â”œâ”€â”€ ğŸ“„ requirements.txt                       # Dependencias Python
â”œâ”€â”€ ğŸ“„ .gitignore                            # Archivos ignorados por Git
â”œâ”€â”€ ğŸ“„ LICENSE                               # Licencia del proyecto
â”œâ”€â”€ ğŸ“„ setup.py                              # ConfiguraciÃ³n de instalaciÃ³n
â”‚
â”œâ”€â”€ ğŸ“ data/                                 # Datos del proyecto
â”‚   â”œâ”€â”€ ğŸ“„ generate_sample_data.py           # Generador de datos sintÃ©ticos
â”‚   â”œâ”€â”€ ğŸ“„ sample_ecommerce_data.csv         # Dataset principal
â”‚   â”œâ”€â”€ ğŸ“„ data_validation.py                # ValidaciÃ³n de calidad de datos
â”‚   â””â”€â”€ ğŸ“ raw/                              # Datos crudos (no procesados)
â”‚
â”œâ”€â”€ ğŸ“ src/                                  # CÃ³digo fuente principal
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ ecommerce_ml_analysis.py          # Clase principal de anÃ¡lisis
â”‚   â”œâ”€â”€ ğŸ“„ data_preprocessing.py             # Preprocesamiento de datos
â”‚   â”œâ”€â”€ ğŸ“„ feature_engineering.py            # IngenierÃ­a de caracterÃ­sticas
â”‚   â”œâ”€â”€ ğŸ“„ model_training.py                 # Entrenamiento de modelos
â”‚   â”œâ”€â”€ ğŸ“„ model_evaluation.py               # EvaluaciÃ³n de modelos
â”‚   â””â”€â”€ ğŸ“„ utils.py                          # Utilidades generales
â”‚
â”œâ”€â”€ ğŸ“ notebooks/                            # Jupyter Notebooks
â”‚   â”œâ”€â”€ ğŸ“„ 01_data_exploration.ipynb         # AnÃ¡lisis exploratorio
â”‚   â”œâ”€â”€ ğŸ“„ 02_feature_engineering.ipynb      # IngenierÃ­a de caracterÃ­sticas
â”‚   â”œâ”€â”€ ğŸ“„ 03_model_development.ipynb        # Desarrollo de modelos
â”‚   â”œâ”€â”€ ğŸ“„ 04_model_evaluation.ipynb         # EvaluaciÃ³n detallada
â”‚   â””â”€â”€ ğŸ“„ ecommerce_prediction_analysis.ipynb # Notebook principal completo
â”‚
â”œâ”€â”€ ğŸ“ scripts/                              # Scripts de automatizaciÃ³n
â”‚   â”œâ”€â”€ ğŸ“„ run_pipeline.py                   # Pipeline principal
â”‚   â”œâ”€â”€ ğŸ“„ train_models.py                   # Script de entrenamiento
â”‚   â”œâ”€â”€ ğŸ“„ evaluate_models.py                # Script de evaluaciÃ³n
â”‚   â”œâ”€â”€ ğŸ“„ deploy_model.py                   # Script de deployment
â”‚   â””â”€â”€ ğŸ“„ monitor_model.py                  # Script de monitoreo
â”‚
â”œâ”€â”€ ğŸ“ models/                               # Modelos entrenados
â”‚   â”œâ”€â”€ ğŸ“ trained_models/                   # Modelos guardados
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ logistic_regression/
â”‚   â”‚   â””â”€â”€ ğŸ“„ random_forest/
â”‚   â”œâ”€â”€ ğŸ“ experiments/                      # Experimentos de hyperparameters
â”‚   â””â”€â”€ ğŸ“„ model_registry.json               # Registro de modelos
â”‚
â”œâ”€â”€ ğŸ“ reports/                              # Reportes y resultados
â”‚   â”œâ”€â”€ ğŸ“„ model_performance_report.md       # Reporte principal
â”‚   â”œâ”€â”€ ğŸ“„ executive_summary.md              # Resumen ejecutivo
â”‚   â”œâ”€â”€ ğŸ“„ technical_documentation.md        # DocumentaciÃ³n tÃ©cnica
â”‚   â”œâ”€â”€ ğŸ“ visualizations/                   # GrÃ¡ficos y visualizaciones
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ model_comparison.png
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ feature_importance.png
â”‚   â”‚   â””â”€â”€ ğŸ“„ confusion_matrix.png
â”‚   â””â”€â”€ ğŸ“ experiments/                      # Reportes de experimentos
â”‚
â”œâ”€â”€ ğŸ“ config/                               # Configuraciones
â”‚   â”œâ”€â”€ ğŸ“„ spark_config.json                # ConfiguraciÃ³n de Spark
â”‚   â”œâ”€â”€ ğŸ“„ model_config.json                # ConfiguraciÃ³n de modelos
â”‚   â””â”€â”€ ğŸ“„ deployment_config.yaml           # ConfiguraciÃ³n de despliegue
â”‚
â”œâ”€â”€ ğŸ“ tests/                                # Tests unitarios
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ test_data_preprocessing.py        # Tests de preprocesamiento
â”‚   â”œâ”€â”€ ğŸ“„ test_feature_engineering.py       # Tests de features
â”‚   â”œâ”€â”€ ğŸ“„ test_model_training.py            # Tests de entrenamiento
â”‚   â””â”€â”€ ğŸ“„ test_model_evaluation.py          # Tests de evaluaciÃ³n
â”‚
â”œâ”€â”€ ğŸ“ deployment/                           # Archivos de despliegue
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile                        # ContainerizaciÃ³n
â”‚   â”œâ”€â”€ ğŸ“„ docker-compose.yml               # OrquestaciÃ³n local
â”‚   â”œâ”€â”€ ğŸ“„ kubernetes_deployment.yaml       # Despliegue K8s
â”‚   â””â”€â”€ ğŸ“ terraform/                        # Infraestructura como cÃ³digo
â”‚
â”œâ”€â”€ ğŸ“ monitoring/                           # Monitoreo y alertas
â”‚   â”œâ”€â”€ ğŸ“„ model_drift_detection.py         # DetecciÃ³n de drift
â”‚   â”œâ”€â”€ ğŸ“„ performance_monitoring.py        # Monitoreo de performance
â”‚   â””â”€â”€ ğŸ“„ alert_system.py                  # Sistema de alertas
â”‚
â”œâ”€â”€ ğŸ“ docs/                                 # DocumentaciÃ³n adicional
â”‚   â”œâ”€â”€ ğŸ“„ api_documentation.md             # DocumentaciÃ³n de API
â”‚   â”œâ”€â”€ ğŸ“„ user_guide.md                    # GuÃ­a de usuario
â”‚   â”œâ”€â”€ ğŸ“„ troubleshooting.md               # SoluciÃ³n de problemas
â”‚   â””â”€â”€ ğŸ“ images/                           # ImÃ¡genes para documentaciÃ³n
â”‚
â””â”€â”€ ğŸ“ logs/                                 # Logs de ejecuciÃ³n
    â”œâ”€â”€ ğŸ“„ pipeline_execution_*.log          # Logs del pipeline
    â”œâ”€â”€ ğŸ“„ model_training_*.log              # Logs de entrenamiento
    â””â”€â”€ ğŸ“„ error_*.log                       # Logs de errores
```

## ğŸ“„ Archivos Principales

### CÃ³digo Fuente

- **`src/ecommerce_ml_analysis.py`**: Clase principal que orquesta todo el anÃ¡lisis
- **`data/generate_sample_data.py`**: Generador de datos sintÃ©ticos realistas
- **`scripts/run_pipeline.py`**: Script principal para ejecutar el pipeline completo
- **`notebooks/ecommerce_prediction_analysis.ipynb`**: AnÃ¡lisis interactivo paso a paso

### ConfiguraciÃ³n

- **`requirements.txt`**: Todas las dependencias necesarias
- **`config/spark_config.json`**: ConfiguraciÃ³n optimizada de Spark
- **`config/model_config.json`**: HiperparÃ¡metros y configuraciÃ³n de modelos

### DocumentaciÃ³n

- **`README.md`**: DocumentaciÃ³n principal del proyecto
- **`reports/model_performance_report.md`**: Reporte detallado de resultados
- **`docs/api_documentation.md`**: DocumentaciÃ³n de la API del modelo

## ğŸš€ Comandos Principales

### InstalaciÃ³n y Setup
```bash
# Clonar repositorio
git clone https://github.com/tu-usuario/mllib-ecommerce-analysis.git
cd mllib-ecommerce-analysis

# Instalar dependencias
pip install -r requirements.txt

# Configurar Spark (opcional, si no estÃ¡ instalado)
./scripts/setup_spark.sh
```

### EjecuciÃ³n del AnÃ¡lisis
```bash
# Pipeline completo (recomendado)
python scripts/run_pipeline.py

# Generar solo datos
python scripts/run_pipeline.py --generate-data --skip-analysis

# Usar datos existentes
python scripts/run_pipeline.py --data-file path/to/your/data.csv

# AnÃ¡lisis interactivo
jupyter notebook notebooks/ecommerce_prediction_analysis.ipynb
```

### Entrenamiento de Modelos
```bash
# Entrenar modelos especÃ­ficos
python scripts/train_models.py --model logistic_regression
python scripts/train_models.py --model random_forest
python scripts/train_models.py --model all

# EvaluaciÃ³n de modelos
python scripts/evaluate_models.py --model-path models/trained_models/
```

### Testing y ValidaciÃ³n
```bash
# Ejecutar tests
pytest tests/

# Tests especÃ­ficos
pytest tests/test_model_training.py -v

# Coverage report
pytest --cov=src tests/
```

## ğŸ“Š Archivos de ConfiguraciÃ³n

### `config/spark_config.json`
```json
{
  "spark.app.name": "EcommerceMLAnalysis",
  "spark.master": "local[*]",
  "spark.sql.adaptive.enabled": "true",
  "spark.sql.adaptive.coalescePartitions.enabled": "true",
  "spark.sql.adaptive.skewJoin.enabled": "true",
  "spark.sql.execution.arrow.pyspark.enabled": "true",
  "spark.serializer": "org.apache.spark.serializer.KryoSerializer",
  "spark.sql.execution.arrow.maxRecordsPerBatch": "10000"
}
```

### `config/model_config.json`
```json
{
  "logistic_regression": {
    "maxIter": [50, 100, 200],
    "regParam": [0.001, 0.01, 0.1],
    "elasticNetParam": [0.0, 0.5, 1.0]
  },
  "random_forest": {
    "numTrees": [50, 100, 150],
    "maxDepth": [5, 10, 15],
    "minInstancesPerNode": [1, 5, 10]
  },
  "cross_validation": {
    "numFolds": 3,
    "seed": 42
  },
  "evaluation_metrics": [
    "areaUnderROC",
    "areaUnderPR",
    "accuracy",
    "f1",
    "precision",
    "recall"
  ]
}
```

## ğŸ”§ Scripts de Utilidad

### `scripts/setup_spark.sh`
```bash
#!/bin/bash
# Script para instalar y configurar Spark localmente

SPARK_VERSION="3.4.0"
HADOOP_VERSION="3"

echo "ğŸ”§ Instalando Apache Spark ${SPARK_VERSION}..."

# Descargar Spark
wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Extraer
tar -xzf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz

# Configurar variables de entorno
export SPARK_HOME=./spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}
export PATH=$PATH:$SPARK_HOME/bin

echo "âœ… Spark instalado exitosamente"
echo "ğŸš€ Para usar: export SPARK_HOME=$PWD/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}"
```

### `scripts/monitor_model.py`
```python
#!/usr/bin/env python3
"""Script de monitoreo continuo del modelo en producciÃ³n"""

import time
import json
import logging
from datetime import datetime
from src.model_evaluation import ModelMonitor

def main():
    monitor = ModelMonitor()
    
    while True:
        try:
            # Verificar performance
            metrics = monitor.check_model_performance()
            
            # Detectar drift
            drift_detected = monitor.detect_data_drift()
            
            # Generar alertas si es necesario
            if drift_detected or metrics['auc'] < 0.75:
                monitor.send_alert(metrics, drift_detected)
            
            # Log estado
            logging.info(f"Model health check: AUC={metrics['auc']:.4f}, Drift={drift_detected}")
            
            # Esperar 1 hora
            time.sleep(3600)
            
        except Exception as e:
            logging.error(f"Error en monitoreo: {e}")
            time.sleep(300)  # Esperar 5 min en caso de error

if __name__ == "__main__":
    main()
```

## ğŸ“¦ Deployment

### `deployment/Dockerfile`
```dockerfile
FROM python:3.9-slim

# Instalar Java (requerido para Spark)
RUN apt-get update && apt-get install -y openjdk-11-jre-headless && rm -rf /var/lib/apt/lists/*

# Configurar directorio de trabajo
WORKDIR /app

# Copiar requirements
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar cÃ³digo fuente
COPY src/ src/
COPY config/ config/
COPY scripts/ scripts/

# Configurar variables de entorno
ENV JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
ENV PYTHONPATH=/app

# Comando por defecto
CMD ["python", "scripts/run_pipeline.py"]
```

### `deployment/docker-compose.yml`
```yaml
version: '3.8'

services:
  mllib-analysis:
    build: .
    container_name: ecommerce-ml-analysis
    volumes:
      - ./data:/app/data
      - ./models:/app/models
      - ./reports:/app/reports
      - ./logs:/app/logs
    environment:
      - SPARK_MASTER=local[*]
      - PYTHONPATH=/app
    ports:
      - "4040:4040"  # Spark UI
    restart: unless-stopped

  model-monitor:
    build: .
    container_name: model-monitor
    command: python scripts/monitor_model.py
    volumes:
      - ./models:/app/models
      - ./logs:/app/logs
    environment:
      - PYTHONPATH=/app
    restart: always
    depends_on:
      - mllib-analysis
```

## ğŸ§ª Testing

### `tests/test_model_training.py`
```python
import unittest
import pandas as pd
from pyspark.sql import SparkSession
from src.ecommerce_ml_analysis import EcommerceMLAnalysis

class TestModelTraining(unittest.TestCase):
    
    @classmethod
    def setUpClass(cls):
        cls.spark = SparkSession.builder.appName("TestMLAnalysis").getOrCreate()
        cls.analyzer = EcommerceMLAnalysis("TestAnalysis")
    
    def test_data_loading(self):
        """Test carga de datos"""
        # Crear datos de prueba
        test_data = pd.DataFrame({
            'customer_id': ['C001', 'C002'],
            'age': [25, 35],
            'will_purchase_next_7_days': [0, 1]
        })
        
        test_data.to_csv('test_data.csv', index=False)
        df = self.analyzer.load_data('test_data.csv')
        
        self.assertEqual(df.count(), 2)
        self.assertIn('customer_id', df.columns)
    
    def test_feature_engineering(self):
        """Test ingenierÃ­a de caracterÃ­sticas"""
        # Implementar test de features
        pass
    
    def test_model_training(self):
        """Test entrenamiento de modelos"""
        # Implementar test de entrenamiento
        pass
    
    @classmethod
    def tearDownClass(cls):
        cls.spark.stop()

if __name__ == '__main__':
    unittest.main()
```

## ğŸ“ˆ Monitoreo y Alertas

### MÃ©tricas Clave a Monitorear
- **Model Performance**: AUC, Precision, Recall
- **Data Drift**: DistribuciÃ³n de caracterÃ­sticas
- **Prediction Drift**: DistribuciÃ³n de predicciones
- **System Health**: Latencia, throughput, errores

### Alertas AutomÃ¡ticas
- AUC < 0.75: Alerta crÃ­tica de degradaciÃ³n
- Data drift > threshold: Alerta de cambio en datos
- Latencia > 500ms: Alerta de performance
- Error rate > 5%: Alerta de estabilidad

## ğŸ”„ CI/CD Pipeline

### `.github/workflows/ci.yml`
```yaml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v3
      with:
        python-version: 3.9
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Run tests
      run: |
        pytest tests/ --cov=src
    
    - name: Run data validation
      run: |
        python data/data_validation.py
    
    - name: Build Docker image
      run: |
        docker build -t mllib-analysis .
```

## ğŸ“š Recursos Adicionales

### Enlaces Ãštiles
- [Apache Spark MLlib Documentation](https://spark.apache.org/mllib/)
- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)
- [Machine Learning Best Practices](https://ml-ops.org/)

### PrÃ³ximos Pasos
1. **Implementar en producciÃ³n** con monitoreo continuo
2. **Agregar mÃ¡s algoritmos** (XGBoost, Deep Learning)
3. **Optimizar pipeline** para datos en tiempo real
4. **Integrar con sistemas existentes** de la empresa

---

**Nota**: Esta estructura proporciona una base sÃ³lida y escalable para el proyecto de Machine Learning con MLlib. Todos los archivos estÃ¡n diseÃ±ados para ser funcionales y seguir las mejores prÃ¡cticas de la industria.